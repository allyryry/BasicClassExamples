{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import thinkstats2\n",
    "import thinkplot\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.stats as ss\n",
    "\n",
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Central Limit Theorem and Non-Normal Data</h1>\n",
    "\n",
    "The Central Limit Theorem is one of the important concepts of statistics, and something that we have address in a backdoor way while doing simulations. The CLT states:\n",
    "<ul>\n",
    "<li>When independent random samples are taken, the distribution of their means approaches normal as more samples are taken. \n",
    "<li>The mean of the sample will equal the mean of the population. \n",
    "<li>The standard deviation of the sample will equal the standard deviation of the population divided by the root of the sample size.\n",
    "</ul>\n",
    "\n",
    "We can use this as a shortcut to help analyze data that is not normal, or where we don't know the distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: the filter here takes out large values. This works fine either way, but with the large outliers in there, visualizing\n",
    "#is less effective, due to the range. \n",
    "# Also, if a sample includes one $20 property randomly, that will have a big impact. \n",
    "df = pd.read_csv(\"Assessments.csv\")\n",
    "df = df[df['Assessed Value']<1000000]\n",
    "df = df[df['Assessed Value']>20000]\n",
    "df.sort_values(\"Assessed Value\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize Distribution and a normality plot\n",
    "thinkplot.PrePlot(2,1,2)\n",
    "sns.histplot(df[\"Assessed Value\"])\n",
    "thinkplot.SubPlot(2)\n",
    "thinkstats2.NormalProbabilityPlot(df[\"Assessed Value\"])\n",
    "thinkplot.Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on both visualizations, the data is pretty clearly not normal. \n",
    "\n",
    "How do we deal with this non-normal data? We have the data of the population here, so we can just calculate it. In reality though, this isn't normally the case. Think about looking at items where we can't capture the population data and are relying on a sample exclusively:\n",
    "<ul>\n",
    "<li>The gorilla example from the book, where we can only tranquilize and assess a small fraction of the total populations.\n",
    "<li>Any survey type data, short of a full census. We only get responses from part of the population.\n",
    "<li>Scientific measurements, where we have only a few examples.\n",
    "</ul>\n",
    "\n",
    "In any of these cases we don't have the distribution of the population, and we really can't get it in any reasonable way. This is where the CLT comes in handy - we can trust the the results of our samples will be normal, then use that to infer about the entire population. The distribution of the means being normal means that we can rely on all the implicit powers of the normal distribution analytical tools.\n",
    "\n",
    "We can call back to our estimation days and generate some samples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take some samples \n",
    "i = 0\n",
    "n = 100\n",
    "total = 1000\n",
    "means = []\n",
    "\n",
    "while i < total:\n",
    "    tmp = df[\"Assessed Value\"].sample(n)\n",
    "    means.append(tmp.mean())\n",
    "    i += 1\n",
    "\n",
    "thinkplot.PrePlot(2,1,2)\n",
    "sns.histplot(means, binwidth=5000, kde=True)\n",
    "thinkplot.SubPlot(2)\n",
    "thinkstats2.NormalProbabilityPlot(means)\n",
    "thinkplot.Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sampling Results</h3>\n",
    "\n",
    "Taking a bunch of samples delivered us a stack of means. We can run some calculations to see how well we did. The means should be pretty close, that's part of the power of the CLT! The standard deviation of the samples are also the standard error, indicating our expected accuracy. If we increase the n, that SE should go down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popMean = df[\"Assessed Value\"].mean()\n",
    "popStd = df[\"Assessed Value\"].std()\n",
    "estMean = np.mean(means)\n",
    "estSE = np.std(means)\n",
    "print(\"Population Mean:\", popMean)\n",
    "print(\"Population Std:\", popStd)\n",
    "print(\"\\n\")\n",
    "print(\"Estimate Mean:\", estMean)\n",
    "print(\"Estimate SE:\", estSE)\n",
    "print(\"Estimate Std:\", estSE*np.sqrt(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Normality status - Increased!</h3>\n",
    "\n",
    "The means of the samples are pretty normal, even if the underlying distribution isn't. The normal distribution is common (hence the normal part), but everything in life isn't normally distributed, as we've seen. \n",
    "<ul>\n",
    "<li>Incomes can often be represented with a lognormal distribution. \n",
    "<li>Property assessments here have a bimodal distribution (two humps).\n",
    "<li>Some things we may want to look at have a distribution that we just don't know at all!\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>CLT in Full Effect</h2>\n",
    "\n",
    "The example above shows the idea behind the CLT - we can take samples and use them to estimate for a population. We are comfortable with this idea from all the estimation stuff. \n",
    "\n",
    "The analytical piece of the CLT is also really handy - we can replicate similar results without all the sampling. Taking 100 or 1000 samples presents similar problems to trying to measure the whole population - it can be hard, time consuming, expensive, or impossible. We can build a function that calculates the CLT directly, and compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculable CLT\n",
    "def clt(data):\n",
    "    mean = np.mean(data)\n",
    "    se = np.std(data) / np.sqrt(len(data))\n",
    "    return (mean, se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run CLT Calculation with 1 Sample\n",
    "#Compare with results above\n",
    "cltRes = clt(df[\"Assessed Value\"].sample(n))\n",
    "print(\"CLT Mean:\", cltRes[0])\n",
    "print(\"CLT SE:\", cltRes[1])\n",
    "print(\"CLT Est Std:\", cltRes[1]*np.sqrt(n))\n",
    "print(\"\\n\")\n",
    "print(\"Population Mean:\", popMean)\n",
    "print(\"Population Std:\", popStd)\n",
    "print(\"\\n\")\n",
    "print(\"Estimate Mean:\", estMean)\n",
    "print(\"Estimate SE:\", estSE)\n",
    "print(\"Estimate Std:\", estSE*np.sqrt(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise\n",
    "\n",
    "#Rerun with a few different N values to see the changes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>CLT is Quick</h3>\n",
    "\n",
    "We can run a bunch of simulations like we did while estimating, but this is way quicker!\n",
    "\n",
    "Try it for some random distribution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate random Exponential Dist - put into a Series for ease of use later. \n",
    "expoDist = pd.Series(ss.expon.rvs(size=10000))\n",
    "sns.histplot(expoDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cltExp = clt(expoDist.sample(n))\n",
    "print(\"CLT Mean:\", cltExp[0])\n",
    "print(\"CLT SE:\", cltExp[1])\n",
    "print(\"CLT Est Std:\", cltExp[1]*np.sqrt(n))\n",
    "print(\"\\n\")\n",
    "print(\"Dist Mean:\", expoDist.mean())\n",
    "print(\"Dist Std:\", expoDist.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the magic of the CLT - we can rely on our smallish sample far more than we might think! \n",
    "\n",
    "Because this works, we can also harness some of the other useful assumptions of normality, like confidence intervals. Going back to the assessments from above, how confident can we be about our estimate of the mean? What is the 90 confidence interval?\n",
    "\n",
    "<h3>Back to the Assessments</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directly calculate confidence intervals.\n",
    "ci1low = ss.norm.ppf(.05, loc=cltRes[0], scale=cltRes[1])\n",
    "ci1hi = ss.norm.ppf(.95, loc=cltRes[0], scale=cltRes[1])\n",
    "ci1low, ci1hi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we compare that to the predictions based on estimation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = thinkstats2.Cdf(means)\n",
    "cdf.Percentile(.05), cdf.Percentile(.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact difference will vary a bit, due to randomness, but we should expect the CI bands to be slightly worse than the estimation example - but we only did one sample! Using the CLT allows us to much more quickly infer about the population when we don't have access to large numbers of samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Try one</h3>\n",
    "\n",
    "Use the CLT to calculate the mean and std from a sample of the salary column below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sal = pd.read_csv(\"spain_salary.csv\")\n",
    "df_sal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data, is it normal? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use CLT to generate mean and std estimates. Compare to a direct calcuation of the population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use CLT to infer mean and std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the 85% confidence interval?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate 85% CI for salary\n",
    "#85% = 7.5 to 92.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Common Transformations - Box Cox</h1>\n",
    "\n",
    "Another strategy to deal with non-normal data is to just do a transformation to make it more normal. We saw this before in one of the sample datasets where we were provided with a log_income value, which changed a lognormal distribution of income into a normal distribution that is easier to analyze. \n",
    "\n",
    "These types of transformations are somewhat of a one-off, what makes a distribution normal will differ. One common method is Box-Cox, which raises values to some value - lambda. E.g. if lambda is 2, the original data is squared, if lambda is .5 the original data is square rooted. Lambda can be between -5 and 5. \n",
    "\n",
    "This process is something that we can utilize scipy to automate - it will give us back a transformation and the optimal lambda. It tests multiple values of lambda, and determines which is most normal using a normality test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load some data\n",
    "def process_time(row):\n",
    "    \n",
    "    call_received = datetime.datetime.strptime(row['date_time_received'].split('+')[0], '%Y-%m-%dT%H:%M:%S')\n",
    "    call_ended    = datetime.datetime.strptime(row['date_time_complete'].split('+')[0], '%Y-%m-%dT%H:%M:%S')\n",
    "    \n",
    "    time_ellapsed = call_ended - call_received\n",
    "\n",
    "    row['Parsed Call Received'] = str(call_received)\n",
    "    row['Parsed Call Ended'] = str(call_ended)\n",
    "    row['Time Ellapsed'] = str(time_ellapsed)\n",
    "    row['Time Ellapsed (minutes)'] = round(time_ellapsed.total_seconds() / 60, 1)\n",
    "\n",
    "    return row\n",
    "\n",
    "import requests\n",
    "import io\n",
    "import datetime\n",
    "base_url = 'https://aegis4048.github.io/downloads/notebooks/sample_data/'\n",
    "filename = '08c32c03-9d88-42a9-b8a1-f493a644b919_NRCEventReporting-Calls-2010.csv'\n",
    "data = requests.get(base_url + filename).content\n",
    "df_   = pd.read_csv(io.StringIO(data.decode('utf-8')))\n",
    "\n",
    "parsed_df = df_.iloc[11000: 12000, :].apply(process_time, axis=1).iloc[:, -4:]\n",
    "parsed_df['Parsed Call Received'] = pd.to_datetime(parsed_df['Parsed Call Received'], format='%Y-%m-%d %H:%M:%S')\n",
    "parsed_df['Parsed Call Ended'] = pd.to_datetime(parsed_df['Parsed Call Ended'], format='%Y-%m-%d %H:%M:%S')\n",
    "parsed_df = parsed_df.sort_values(by = 'Parsed Call Received')\n",
    "\n",
    "new_parsed_df = parsed_df.drop_duplicates(subset=['Parsed Call Received'], keep=False)\n",
    "df2 = new_parsed_df.copy()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize call time\n",
    "calls = pd.Series(df2[\"Time Ellapsed (minutes)\"])\n",
    "sns.histplot(calls, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transCalls, lmbda = ss.boxcox(calls)\n",
    "print(lmbda)\n",
    "sns.histplot(transCalls, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much more normal!\n",
    "\n",
    "<h3>Application - Control Charts</h3>\n",
    "\n",
    "One use of a transformation like this would be to monitor and detect things that fall outside the normal expected bounds. This data is from a call center, we may want to see when calls fall outside a specified range. Going in however, we don't have a cutoff - we can use the distribution to tell us how many are outside of 3std. This is common in lots of quality control/manufacturing types of scenarios. \n",
    "\n",
    "The control chart has a normality assumption, because the idea behind it is based off of the standard deviation. We want to \"catch\" things that are over X standard deviations from normal. If you've heard of Six Sigma, that's based on the same idea - to manufacture things to \"six sigmas\" of quality (or without error): roughly 99.99966%. This type of transformation to normal for a control chart is a common thing. For more info: https://www.isixsigma.com/tools-templates/control-charts/non-normal-data-needs-alternate-control-chart-approach/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set # of standard deviations to draw control lines. \n",
    "limit = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-transformed control chart. \n",
    "y = new_parsed_df['Parsed Call Received'].values\n",
    "mean = np.mean(calls)\n",
    "std  = np.std(calls)\n",
    "\n",
    "upper_limit = mean + limit * std\n",
    "lower_limit = mean - limit * std\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "ax.plot(y, calls)\n",
    "\n",
    "ax.axhline(mean, color='C1')\n",
    "ax.axhline(upper_limit, color='r')\n",
    "ax.axhline(lower_limit, color='r');\n",
    "ax.text(y[-200], upper_limit + 3, 'Upper Control Limit', color='r')\n",
    "ax.text(y[-200], lower_limit + 3, 'Lower Control Limit', color='r')\n",
    "ax.text(y[3], mean + 3, 'Mean', color='C1')\n",
    "ax.set_ylabel('Call duration (minutes)');\n",
    "ax.set_title('Control Chart for Phone Call Duration - Original');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't super helpful. The lower limits are negative, the control limits seem random. Not great. \n",
    "\n",
    "We can make one with transformed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformed control chart. \n",
    "mean_trans = np.mean(transCalls)\n",
    "std_trans  = np.std(transCalls)\n",
    "\n",
    "upper_limit_trans = mean_trans + limit * std_trans\n",
    "lower_limit_trans = mean_trans - limit * std_trans\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "ax.plot(y, transCalls)\n",
    "\n",
    "ax.axhline(mean_trans, color='C1')\n",
    "ax.axhline(upper_limit_trans, color='r')\n",
    "ax.axhline(lower_limit_trans, color='r');\n",
    "ax.text(y[-200], upper_limit_trans - 0.15, 'Upper Control Limit', color='r')\n",
    "ax.text(y[-200], lower_limit_trans + 0.15, 'Lower Control Limit', color='r')\n",
    "ax.text(y[3], mean_trans + 0.1, 'Mean', color='C1')\n",
    "ax.set_ylabel('Call duration (minutes)');\n",
    "ax.set_title('Control Chart for Phone Call Duration - Transformed');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Way better! Now the control lines are useful. This is something that is really done in practice. \n",
    "\n",
    "Now, we don't really care about the transformed data specifically, we care about the real values. How do we go backwards? Call the inverse transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reverse the transformation. \n",
    "from scipy.special import inv_boxcox\n",
    "backTrans = inv_boxcox(transCalls, lmbda)\n",
    "sns.histplot(backTrans, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! Back where we started. \n",
    "\n",
    "Why? We can transform some data to make it normal to utilize any normal-based analysis on the data. We can more easily do hypothesis testing, control charts, confidence intervals, etc...\n",
    "\n",
    "Using the benefits of normality can help us out!\n",
    "\n",
    "Another similar example is Yeo-Johnson: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.yeojohnson.html \n",
    "\n",
    "There's also QunitileTransformer, but it requires far more data: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html#sklearn.preprocessing.QuantileTransformer "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea39297c2a3b8433e0e3c4b620aff79df88eb4bda961dfb2311fbafd7efdbd77"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
